{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGLkmkoEfWOx+qzKReQcUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akansh30/NCERT-BOT/blob/main/DataExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrAiM4FL-Kjy",
        "outputId": "8075c055-fde0-4f58-c4f4-fcd6dfd6dead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (4.0.2)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.0.3)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->fitz) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (67.7.2)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.3)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson>=3.8.0 (from nipype->fitz)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.15.4)\n",
            "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Collecting looseversion (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.1)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.31.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2024.7.4)\n",
            "Installing collected packages: looseversion, traits, simplejson, isodate, configparser, configobj, ci-info, rdflib, pyxnat, etelemetry, prov, nipype, fitz\n",
            "Successfully installed ci-info-0.3.0 configobj-5.0.8 configparser-7.0.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 prov-2.0.1 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.2 traits-6.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install fitz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZhmlKZp-R3-",
        "outputId": "2ddc7f2e-fb2f-4d28-e2b9-64c46488b3da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.6 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.6 pymupdf-1.24.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSSNfDPz-XnX",
        "outputId": "8c0bc19e-2e86-4ede-995d-13299303963d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your folder is named 'eng_ncert' in MyDrive\n",
        "english_folder_dir = '/content/drive/MyDrive/eng_ncert/'\n"
      ],
      "metadata": {
        "id": "B7WNqYmF-fXT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import fitz\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Mount Google Drive (replace 'MyDrive' if needed)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update the path to reflect the mounted Google Drive location\n",
        "english_folder_dir = '/content/drive/MyDrive/eng_ncert/'  # Path to your 'eng_ncert' folder\n",
        "\n",
        "# Define the function before it's called\n",
        "def get_text_from_papers(filename: str, get_html: bool = False):\n",
        "    all_page_data = {}\n",
        "    doc = fitz.open(filename)\n",
        "    text = \"\"\n",
        "    for i, page in enumerate(doc):\n",
        "        all_page_data[i] = {}\n",
        "        all_page_data[i]['text'] = (\"\\n\" + (text := page.get_text)(\"\"))\n",
        "        if get_html:\n",
        "            soup = BeautifulSoup(text('html'))\n",
        "            all_page_data[i]['html'] = soup.prettify()\n",
        "    return all_page_data\n",
        "\n",
        "books_name = os.listdir(english_folder_dir)\n",
        "\n",
        "# Loop through each book directory\n",
        "for book in books_name:\n",
        "    book_path = os.path.join(english_folder_dir, book)\n",
        "    files = os.listdir(book_path)\n",
        "\n",
        "    # Loop through each PDF file in the book directory\n",
        "    for filename in files:\n",
        "        if filename.endswith('.pdf'):  # Check if it's a PDF file\n",
        "            file_path = os.path.join(book_path, filename)\n",
        "            data = get_text_from_papers(file_path, True) # Now you can call the function\n",
        "            # Process the extracted text and HTML data (explained earlier)\n",
        "            print(f\"Extracted data from {book}/{filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbPEPqE1-ktT",
        "outputId": "e64ef4e1-cebe-4b24-bcce-a9dfa3cdad50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted data from supplyeng/kesp102.pdf\n",
            "Extracted data from supplyeng/kesp105.pdf\n",
            "Extracted data from supplyeng/kesp103.pdf\n",
            "Extracted data from supplyeng/kesp1ps.pdf\n",
            "Extracted data from supplyeng/kesp101.pdf\n",
            "Extracted data from supplyeng/kesp104.pdf\n",
            "Extracted data from hornbill/kehb1ps.pdf\n",
            "Extracted data from hornbill/kehb105.pdf\n",
            "Extracted data from hornbill/kehb101.pdf\n",
            "Extracted data from hornbill/kehb102.pdf\n",
            "Extracted data from hornbill/kehb113.pdf\n",
            "Extracted data from hornbill/kehb104.pdf\n",
            "Extracted data from hornbill/kehb114.pdf\n",
            "Extracted data from hornbill/kehb106.pdf\n",
            "Extracted data from hornbill/kehb115.pdf\n",
            "Extracted data from hornbill/kehb112.pdf\n",
            "Extracted data from hornbill/kehb103.pdf\n",
            "Extracted data from hornbill/kehb111.pdf\n",
            "Extracted data from hornbill/kehb116.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_text_from_papers(filename: str):\n",
        "  \"\"\"\n",
        "  Extracts text content from all pages of a PDF.\n",
        "\n",
        "  Args:\n",
        "      filename (str): Path to the PDF file.\n",
        "\n",
        "  Returns:\n",
        "      str: Concatenated text content from all pages.\n",
        "  \"\"\"\n",
        "\n",
        "  all_text = \"\"\n",
        "  try:\n",
        "      doc = fitz.open(filename)\n",
        "      for page in doc:\n",
        "          text = page.get_text(\"text\")\n",
        "          all_text += text + \"\\n\\n\"  # Add newline separators between pages\n",
        "      return all_text\n",
        "  except Exception as e:\n",
        "      print(f\"Error extracting text from {filename}: {e}\")\n",
        "      return \"\"\n",
        "\n",
        "def extract_with_regex(text: str, heading: str):\n",
        "  \"\"\"\n",
        "  Extracts text following a specific heading using regex.\n",
        "\n",
        "  Args:\n",
        "      text (str): The text content to search.\n",
        "      heading (str): The heading to identify (case-sensitive).\n",
        "\n",
        "  Returns:\n",
        "      str: The text found after the specified heading, or an empty string.\n",
        "  \"\"\"\n",
        "\n",
        "  pattern = rf\"{heading}\\n(?:\\d+\\.\\s+)?(.*?)(?=^|\\n\\b{heading}\\b|\\n\\b[A-Z][^A-Z\\d\\s]*\\b)\"  # Regular expression pattern\n",
        "  match = re.search(pattern, text, flags=re.DOTALL)  # Multiline search\n",
        "  if match:\n",
        "      return match.group(1).strip()  # Extract text after the heading\n",
        "  else:\n",
        "      return \"\"\n",
        "\n",
        "# Assuming english_folder_dir points to your root directory\n",
        "english_folder_dir = '/content/drive/MyDrive/eng_ncert'  # Replace with your actual path\n",
        "\n",
        "# Target folder (hornbill) and filename (kehb103.pdf)\n",
        "target_folder = \"hornbill\"\n",
        "target_filename = \"kehb103.pdf\"\n",
        "\n",
        "# Construct the complete file path\n",
        "file_path = os.path.join(english_folder_dir, target_folder, target_filename)\n",
        "\n",
        "extracted_text = get_text_from_papers(file_path)\n",
        "if extracted_text:\n",
        "  understanding_text = extract_with_regex(extracted_text, \"Understanding the text\")  # Search for the heading with lowercase \"t\"\n",
        "  if understanding_text:\n",
        "      print(understanding_text)\n",
        "  else:\n",
        "      print(f\"Content under 'Understanding the text' not found in {target_filename}.\")\n",
        "else:\n",
        "  print(f\"Error extracting text from {file_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD5HuX_iT2K1",
        "outputId": "d0de2698-f61d-434a-cb2a-82d1fb4eec11"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give reasons for the following.\n",
            "(i) King Tut’s body has been subjected to repeated scrutiny.\n",
            "(ii) Howard Carter’s investigation was resented.\n",
            "(iii) Carter had to chisel away the solidified resins to raise the\n",
            "king’s remains.\n",
            "(iv) Tut’s body was buried along with gilded treasures.\n",
            "(v) The boy king changed his name from Tutankhaten to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_text_by_number(filename: str):\n",
        "  \"\"\"\n",
        "  Extracts text under numbered sections (1., 2., etc.) in a PDF.\n",
        "\n",
        "  Args:\n",
        "      filename (str): Path to the PDF file.\n",
        "\n",
        "  Returns:\n",
        "      str: The extracted text under numbered sections, separated by newlines.\n",
        "  \"\"\"\n",
        "\n",
        "  all_text = \"\"\n",
        "  section_pattern = r\"^\\d+\\.\\s+(.*?)(?=\\n|\\Z)\"  # Match lines starting with number, dot, and capture text until newline or end\n",
        "  try:\n",
        "      doc = fitz.open(filename)\n",
        "      for page in doc:  # Iterate through all pages\n",
        "          page_text = page.get_text(\"text\")\n",
        "          for line in page_text.splitlines():\n",
        "              match = re.match(section_pattern, line)\n",
        "              if match:\n",
        "                  section_text = match.group(1).strip()\n",
        "                  all_text += f\"{section_text}\\n\\n\"  # Add newline separators between sections\n",
        "      return all_text.strip()  # Remove any trailing newline\n",
        "  except Exception as e:\n",
        "      print(f\"Error extracting content from {filename}: {e}\")\n",
        "      return \"\"\n",
        "\n",
        "# Assuming english_folder_dir points to your root directory\n",
        "english_folder_dir = '/content/drive/MyDrive/eng_ncert'  # Replace with your actual path\n",
        "\n",
        "# Target folder (hornbill) and filename (kehb103.pdf)\n",
        "target_folder = \"hornbill\"\n",
        "target_filename = \"kehb103.pdf\"\n",
        "\n",
        "# Construct the complete file path\n",
        "file_path = os.path.join(english_folder_dir, target_folder, target_filename)\n",
        "\n",
        "text_by_number = get_text_by_number(file_path)\n",
        "\n",
        "if text_by_number:\n",
        "  print(text_by_number)\n",
        "else:\n",
        "  print(f\"No text found under numbered sections in {target_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7xVr4EQu_ts",
        "outputId": "ce8b567c-f5b9-438f-ca00-038c3fc391a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text found under numbered sections in kehb103.pdf.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "\n",
        "def get_all_text_by_page(filename: str, page_numbers: list[int]):\n",
        "  \"\"\"\n",
        "  Extracts all text from specified pages of a PDF.\n",
        "\n",
        "  Args:\n",
        "      filename (str): Path to the PDF file.\n",
        "      page_numbers (list[int]): A list of page numbers to search.\n",
        "\n",
        "  Returns:\n",
        "      str: The combined text extracted from all specified pages.\n",
        "  \"\"\"\n",
        "\n",
        "  all_text = \"\"\n",
        "  try:\n",
        "      doc = fitz.open(filename)\n",
        "      for page_number in page_numbers:\n",
        "          page = doc[page_number - 1]  # Indexing starts from 0\n",
        "          page_text = page.get_text(\"text\")\n",
        "          all_text += page_text + \"\\n\\n\"  # Add newline separators between pages\n",
        "      return all_text\n",
        "  except Exception as e:\n",
        "      print(f\"Error extracting content from {filename}: {e}\")\n",
        "      return \"\"\n",
        "\n",
        "# Assuming english_folder_dir points to your root directory\n",
        "english_folder_dir = '/content/drive/MyDrive/eng_ncert'  # Replace with your actual path\n",
        "\n",
        "# Target folder (hornbill) and filename (kehb1ps.pdf)\n",
        "target_folder = \"hornbill\"\n",
        "target_filename = \"kehb1ps.pdf\"\n",
        "\n",
        "# Page numbers to extract text from (replace with desired pages)\n",
        "page_numbers = [11, 12]\n",
        "\n",
        "# Construct the complete file path\n",
        "file_path = os.path.join(english_folder_dir, target_folder, target_filename)\n",
        "\n",
        "all_text = get_all_text_by_page(file_path, page_numbers)\n",
        "\n",
        "if all_text:\n",
        "  print(all_text)\n",
        "else:\n",
        "  print(f\"No text found on pages {page_numbers} in {target_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMlBhhVwm5ym",
        "outputId": "a394223e-3dbb-446c-c5e7-c17f785a4895"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents\n",
            "FOREWORD\n",
            "iii\n",
            "RATIONALISATION OF CONTENT IN THE TEXTBOOKS\n",
            "V\n",
            "ABOUT THE BOOK\n",
            "vii\n",
            "READING SKILLS\n",
            "1–72\n",
            "1.\n",
            "The Portrait of a Lady\n",
            "3\n",
            "KHUSHWANT SINGH\n",
            "A Photograph\n",
            "11\n",
            "SHIRLEY TOULSON\n",
            "2.\n",
            "We’re Not Afraid to Die...\n",
            "if We Can All Be Together\n",
            "13\n",
            "GORDON COOK and ALAN EAST\n",
            "3.\n",
            "Discovering Tut: the\n",
            "Saga Continues\n",
            "22\n",
            "A.R. WILLIAMS\n",
            "The Laburnum Top\n",
            "31\n",
            "TED HUGHES\n",
            "The Voice of the Rain\n",
            "34\n",
            "WALT WHITMAN\n",
            "4.\n",
            "The Ailing Planet: the\n",
            "Green Movement’s Role\n",
            "36\n",
            "NANI PALKHIVALA\n",
            "Childhood\n",
            "43\n",
            "MARKUS NATTEN\n",
            "2024-25\n",
            "\n",
            "\n",
            "5.\n",
            "The Adventure\n",
            "45\n",
            "JAYANT NARLIKAR\n",
            "6.\n",
            "Silk Road\n",
            "59\n",
            "NICK MIDDLETON\n",
            "Father to Son\n",
            "71\n",
            "ELIZABETH JENNINGS\n",
            "WRITING SKILLS\n",
            "73–104\n",
            "1.\n",
            "Note-making\n",
            "75\n",
            "2.\n",
            "Summarising\n",
            "80\n",
            "3.\n",
            "Sub-titling\n",
            "85\n",
            "4.\n",
            "Essay-writing\n",
            "88\n",
            "5.\n",
            "Letter-writing\n",
            "93\n",
            "6.\n",
            "Creative Writing\n",
            "102\n",
            "xii\n",
            "2024-25\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQ-fM6lPu7FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fk55EFDNqf_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_ordered_content_by_page(filename: str, page_numbers: list[int]):\n",
        "  \"\"\"\n",
        "  Extracts all text from specified pages, attempting to order chapters.\n",
        "\n",
        "  Args:\n",
        "      filename (str): Path to the PDF file.\n",
        "      page_numbers (list[int]): A list of page numbers to search.\n",
        "\n",
        "  Returns:\n",
        "      str: The extracted and (potentially) ordered text.\n",
        "  \"\"\"\n",
        "\n",
        "  all_text = \"\"\n",
        "  chapter_pattern = r\"^\\d+\\.\\s+(.+?)\\s*$\"  # Match chapter lines (number, dot, name)\n",
        "  try:\n",
        "      doc = fitz.open(filename)\n",
        "      chapters = []\n",
        "      for page_number in page_numbers:\n",
        "          page = doc[page_number - 1]  # Indexing starts from 0\n",
        "          page_text = page.get_text(\"text\")\n",
        "          lines = page_text.splitlines()\n",
        "          for line in lines:\n",
        "              chapter_match = re.match(chapter_pattern, line)\n",
        "              if chapter_match:\n",
        "                  chapter_name = chapter_match.group(1).strip()\n",
        "                  chapters.append((chapter_name, page_number))\n",
        "              else:\n",
        "                  all_text += line + \"\\n\"  # Append non-chapter lines\n",
        "      # Attempt to order chapters based on page numbers\n",
        "      chapters.sort(key=lambda x: x[1])\n",
        "      formatted_text = \"\"\n",
        "      chapter_number = 1\n",
        "      for chapter_name, page_number in chapters:\n",
        "          # Check if chapter number needs incrementing\n",
        "          if chapter_number != page_number:\n",
        "              chapter_number = page_number\n",
        "          formatted_text += f\"{chapter_number}. {chapter_name}\\n\"\n",
        "      formatted_text += all_text.strip()  # Add remaining non-chapter text\n",
        "      return formatted_text\n",
        "  except Exception as e:\n",
        "      print(f\"Error extracting content from {filename}: {e}\")\n",
        "      return \"\"\n",
        "\n",
        "# Assuming english_folder_dir points to your root directory\n",
        "english_folder_dir = '/content/drive/MyDrive/eng_ncert'  # Replace with your actual path\n",
        "\n",
        "# Target folder (hornbill) and filename (kehb1ps.pdf)\n",
        "target_folder = \"hornbill\"\n",
        "target_filename = \"kehb1ps.pdf\"\n",
        "\n",
        "# Page numbers to extract text from\n",
        "page_numbers = [11, 12]\n",
        "\n",
        "# Construct the complete file path\n",
        "file_path = os.path.join(english_folder_dir, target_folder, target_filename)\n",
        "\n",
        "ordered_text = get_ordered_content_by_page(file_path, page_numbers)\n",
        "\n",
        "if ordered_text:\n",
        "  print(ordered_text)\n",
        "else:\n",
        "  print(f\"No text found on pages {page_numbers} in {target_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNvCbx90qhvc",
        "outputId": "7eae1ecf-fbd4-4da3-ec0b-d23c324f904e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents\n",
            "FOREWORD\n",
            "iii\n",
            "RATIONALISATION OF CONTENT IN THE TEXTBOOKS\n",
            "V\n",
            "ABOUT THE BOOK\n",
            "vii\n",
            "READING SKILLS\n",
            "1–72\n",
            "1.\n",
            "The Portrait of a Lady\n",
            "3\n",
            "KHUSHWANT SINGH\n",
            "A Photograph\n",
            "11\n",
            "SHIRLEY TOULSON\n",
            "2.\n",
            "We’re Not Afraid to Die...\n",
            "if We Can All Be Together\n",
            "13\n",
            "GORDON COOK and ALAN EAST\n",
            "3.\n",
            "Discovering Tut: the\n",
            "Saga Continues\n",
            "22\n",
            "A.R. WILLIAMS\n",
            "The Laburnum Top\n",
            "31\n",
            "TED HUGHES\n",
            "The Voice of the Rain\n",
            "34\n",
            "WALT WHITMAN\n",
            "4.\n",
            "The Ailing Planet: the\n",
            "Green Movement’s Role\n",
            "36\n",
            "NANI PALKHIVALA\n",
            "Childhood\n",
            "43\n",
            "MARKUS NATTEN\n",
            "2024-25\n",
            "5.\n",
            "The Adventure\n",
            "45\n",
            "JAYANT NARLIKAR\n",
            "6.\n",
            "Silk Road\n",
            "59\n",
            "NICK MIDDLETON\n",
            "Father to Son\n",
            "71\n",
            "ELIZABETH JENNINGS\n",
            "WRITING SKILLS\n",
            "73–104\n",
            "1.\n",
            "Note-making\n",
            "75\n",
            "2.\n",
            "Summarising\n",
            "80\n",
            "3.\n",
            "Sub-titling\n",
            "85\n",
            "4.\n",
            "Essay-writing\n",
            "88\n",
            "5.\n",
            "Letter-writing\n",
            "93\n",
            "6.\n",
            "Creative Writing\n",
            "102\n",
            "xii\n",
            "2024-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5S4I68Hro87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}